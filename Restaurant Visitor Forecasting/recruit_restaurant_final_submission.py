# -*- coding: utf-8 -*-
"""Recruit_Restaurant_Final_Submission.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jQguhNN6yNR7vsdD0w8JbENGSPyiC0Ks

## Imports and loading data
"""

#import python libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xg
import lightgbm as lgb
#import catboost 
from datetime import datetime as dt
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_log_error, r2_score
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import train_test_split 
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import MinMaxScaler

#mounting google drive to load data from it
from google.colab import drive
drive.mount('/content/gdrive')

cd gdrive/My Drive/Colab Notebooks/recruit

path=""

# Read csv files
air_reserve = pd.read_csv(path + 'air_reserve.csv')
air_reserve.name="air_reserve"
air_store_info = pd.read_csv(path + 'air_store_info.csv')
air_store_info.name="air_store_info"
air_visit_data = pd.read_csv(path + 'air_visit_data.csv')
air_visit_data.name="air_visit_data"
hpg_reserve = pd.read_csv(path + 'hpg_reserve.csv')
hpg_reserve.name="hpg_reserve"
hpg_store_info = pd.read_csv(path + 'hpg_store_info.csv')
hpg_store_info.name="hpg_store_info"
store_id_relation = pd.read_csv(path + 'store_id_relation.csv')
store_id_relation.name="store_relation_id"
date_info = pd.read_csv(path + 'date_info.csv')
date_info.name="date_info"
sample_sub = pd.read_csv(path + 'sample_submission.csv')

"""## EDA

###### Checking for Missing Data
"""

listofdf=[air_reserve,air_store_info,date_info,hpg_reserve,hpg_store_info,store_id_relation]
for df in listofdf:
    print(df.isnull().sum(), "\n")

"""No missing data was found

###### Stats of each dataset
"""

for df in listofdf:
    print(df.name,"\n")
    print(df.describe(),"\n")

"""#### Visualization

###### *Reservation data*
"""

airres=air_reserve.copy()
hpgres=hpg_reserve.copy()

#convert visit date and time to separate date and time columns 
airres['Dates'] = pd.to_datetime(airres['visit_datetime']).dt.date
airres['Time'] = pd.to_datetime(airres['visit_datetime']).dt.time

hpgres['Dates'] = pd.to_datetime(hpgres['visit_datetime']).dt.date
hpgres['Time'] = pd.to_datetime(hpgres['visit_datetime']).dt.time

#group the reservations by date to check the reservation trend
group_date_air=airres.groupby(['Dates']).sum()
group_date_hpg=hpgres.groupby(['Dates']).sum()

#plot reservation visitors vs date from airreserve
sns.set(rc={'figure.figsize':(15,5)})
group_date_air['reserve_visitors'].plot(linewidth=2);

#There is missing data from July 2016 to November 2016 after which there is a huge spike of visitors. 
#New years time may have more number of people who go out for dining

#plot reservation visitors vs date from hpg_reserve
sns.set(rc={'figure.figsize':(15,5)})
group_date_hpg['reserve_visitors'].plot(linewidth=2);

#Here again there is spike in January which might suggest the New Years time is popular for Dining out

"""##### *Holiday info*"""

date=date_info.copy()

date['holiday_flg']=date['holiday_flg'].astype(int)
datehol=date.loc[date['holiday_flg']==1]

datehol.groupby('day_of_week').sum()

sns.countplot(data=datehol.groupby('day_of_week').mean(),x=datehol.day_of_week)
#count of holidays vs day of week
#monday has the hightest number of holidays

"""##### *Store id vs visitors*"""

air_visit=air_visit_data.copy()
# air_Visit group by air_store_id
group_id_air_visit=air_visit.groupby(['air_store_id']).sum()
group_id_air_visit=group_id_air_visit.sort_values(by='visitors',ascending=False)
group_id_air_visit.plot()

#air store id vs total no of visitors

"""######*Number of visitors and time*"""

visitors_time=airres[['Time','reserve_visitors']]
group_visitors_time=visitors_time.groupby('Time').sum()
group_visitors_time.plot()

# time of day vs total number of visitors
# we can see the number of visitors peak at after 6pm

"""##### *Area and Genre*"""

air_res_store=pd.merge(airres,air_store_info,on='air_store_id', how='inner')
air=pd.merge(air_res_store,air_visit,on='air_store_id',how='inner')

g_genre=air.groupby('air_genre_name')
df=g_genre['visitors'].count().sort_values().plot(kind='bar',align='center')
# Genre vs Number of visitors

#visitors based on area name
g_genre=air.groupby('air_area_name')
df=g_genre['visitors'].count().sort_values().plot(kind='bar',align='center')
# area vs no of visitors

#top 15 areas with max restaurants
temp=air_store_info.air_area_name.value_counts()
temp[:15].sort_values().plot(kind='bar')

visits_dates=pd.merge(air_visit,date,left_on='visit_date', right_on='calendar_date',how='left')

"""##### *Trend of visitors if its a holiday*"""

visits_dates_grp=visits_dates.loc[visits_dates['holiday_flg']==1]
sns.countplot(data=visits_dates_grp,x=visits_dates_grp.day_of_week)
#mondays are popular if its a holiday

"""##### *Trend of visitors if its not a holiday*"""

visits_dates_grp_normal=visits_dates.loc[visits_dates['holiday_flg']==0]
sns.countplot(data=visits_dates_grp_normal,x=visits_dates_grp_normal.day_of_week)
#friday is the most popular choice for a non holiday while monday is the least popular

"""##### *Genre and date*"""

visits_genre=pd.merge(air_visit,air_store_info,left_on='air_store_id', right_on='air_store_id',how='left')
visits_genre=visits_genre.drop(['latitude', 'longitude','air_area_name','air_store_id'], axis=1)

visits_genre['visit_date'] = pd.to_datetime(visits_genre.visit_date)
visits_genre['visit_date'] = visits_genre['visit_date'].dt.strftime('%Y-%m')
visits_genre_grp=visits_genre.groupby('air_genre_name').count()

visits_genre_grp_sum=visits_genre.groupby('air_genre_name').sum()
visits_genre_grp_sum['Count of each cuisine'] = visits_genre_grp['visitors']
visits_genre_grp_sum['Avg Visitors'] = visits_genre_grp_sum['visitors']/visits_genre_grp_sum['Count of each cuisine']
visits_genre_grp_sum.sort_values(by=['Avg Visitors'], ascending=False)

visits_genre.groupby(['visit_date']).sum().plot(linewidth=2)

"""## Feature Engineering

###### *Reservation Data*
"""

# combine air and hpg Reserve databases 
hpg_air_reserve = store_id_relation.join(hpg_reserve.set_index('hpg_store_id'), on = 'hpg_store_id')
temp_air_reserve = air_reserve.copy()
#dropping hpg_store_id since its counter air_store_id is present
hpg_air_reserve = hpg_air_reserve.drop('hpg_store_id', axis = 1)
#concatenating the entire reserve data together 
reserve = pd.concat([temp_air_reserve, hpg_air_reserve])

# convert columns of "reserve" table into datetime format
reserve['visit_datetime'] =  pd.to_datetime(reserve['visit_datetime'])
reserve['reserve_datetime'] =  pd.to_datetime(reserve['reserve_datetime'])

# create column for visit date inside "reserve" table (since air_visit_data has the visit_date only and not the time, we'll need this columns while merging)
reserve['visit_date'] = reserve['visit_datetime'].apply(lambda x: str(x)[0:10])

# calculate the gap between visit time and reservation time inside "reserve" table
reserve['hour_gap'] = reserve['visit_datetime'].sub(reserve['reserve_datetime'])
reserve['hour_gap'] = reserve['hour_gap'].apply(lambda x: x/np.timedelta64(1,'h'))

# separate reservation into 5 categories based on gap length
# gaps are of length (0.5 day, 1 day, 1 day, 1 day, remaining time) 
reserve['reserve_-12_h'] = np.where(reserve['hour_gap'] <= 12,
                                    reserve['reserve_visitors'], 0)
reserve['reserve_12_37_h'] = np.where((reserve['hour_gap'] <= 37) & (reserve['hour_gap'] > 12),
                                       reserve['reserve_visitors'], 0)
reserve['reserve_37_59_h'] = np.where((reserve['hour_gap'] <= 59) & (reserve['hour_gap'] > 37),
                                       reserve['reserve_visitors'], 0)
reserve['reserve_59_85_h'] = np.where((reserve['hour_gap'] <= 85) & (reserve['hour_gap'] > 59),
                                       reserve['reserve_visitors'], 0)
reserve['reserve_85+_h'] = np.where((reserve['hour_gap'] > 85),
                                     reserve['reserve_visitors'], 0)

# group by air_store_id and visit_date to enable joining with main table
group_list = ['air_store_id', 'visit_date', 'reserve_visitors', 'reserve_-12_h',
              'reserve_12_37_h', 'reserve_37_59_h', 'reserve_59_85_h', 'reserve_85+_h']

reserve = reserve[group_list].groupby(['air_store_id', 'visit_date'], as_index = False).sum()

"""###### Genre and Area"""

# total amount of restaurants of specific genres by area_name 
# group by air genre and area name
air_genres_in_area = air_store_info.copy()
air_genres_in_area = air_genres_in_area[['air_store_id', 'air_genre_name', 'air_area_name']].groupby(['air_genre_name', 'air_area_name'],
                                                                                              as_index = False).count()
air_genres_in_area = air_genres_in_area.rename(columns = {'air_store_id': 'genre_in_area'})

# merge df with air_genres_in_area to ge the total restaurants of same genre in an area, with respect to store id
def merge_genres_area(df):
    df = pd.merge(df, air_genres_in_area, how = 'left',left_on = ['air_genre_name', 'air_area_name'],right_on = ['air_genre_name', 'air_area_name'])
    return df

# total amount of restaurants in area 
# group by area name and count the no of restaurants in that area
air_area_total_ret = air_store_info.copy()
air_area_total_ret = air_area_total_ret[['air_store_id', 'air_area_name']].groupby(['air_area_name'], as_index = False).count()
air_area_total_ret = air_area_total_ret.rename(columns = {'air_store_id': 'total_rt_in_area'})

# merge df with air_area_total_ret to get the total number of restaurants in an area with respect to store id
def merge_total_rt_in_area(df):
  df = pd.merge(df, air_area_total_ret, how = 'left',left_on = ['air_area_name'],right_on = ['air_area_name'])
  return df;

"""###### *Holidays*"""

# additional features for weekends and holidays
date_info_mod = date_info.copy()
date_info_mod['holiday_eve'] = np.zeros(date_info_mod.shape[0])

# holiday eve feature created which marks whether the next day is a holiday or not
date_info_mod['holiday_eve'].iloc[:-1] = date_info_mod['holiday_flg'].copy().values[1:]
date_info_mod.head(4)

# non_working_day feature indicates whether it is a working or non-working day. Includes holidays along with weekends
date_info_mod['non_working_day'] = np.where(date_info_mod['day_of_week'].isin(['Saturday', 'Sunday']) |date_info_mod['holiday_flg'] == 1, 1, 0)
date_info_mod.head(4)

# drop redundant feature holiday_flag
date_info_mod = date_info_mod.drop('holiday_flg', axis = 1)

# joining air_visit_data with the date_info we created above
air_visit_wd = air_visit_data.join(date_info_mod.set_index('calendar_date'), on = 'visit_date')
air_visit_wd.head()

# average visitors per restaurant by working and non-working days
# average visitors per restaurant
mean_of_visitors = air_visit_wd[['visitors','air_store_id','non_working_day']].copy().groupby(['air_store_id','non_working_day'],as_index = False).mean()
mean_of_visitors = mean_of_visitors.rename(columns = {'visitors': 'nwd_mean_visitors'})


# median visitors per restaurant
median_of_visitors = air_visit_wd[['visitors','air_store_id','non_working_day']].copy().groupby(['air_store_id','non_working_day'],as_index = False).median()
median_of_visitors = median_of_visitors.rename(columns = {'visitors': 'nwd_median_visitors'})


# max visitors per restaurant
max_visitors = air_visit_wd[['visitors','air_store_id','non_working_day']].copy().groupby(['air_store_id','non_working_day'],as_index = False).max()
max_visitors = max_visitors.rename(columns = {'visitors': 'nwd_max_visitors'})

# merge dataframe with the calculated data -> mean median and max visitors as per restaurant
def merge_calc_wd_data(data, calc_data):
    data = pd.merge(data, calc_data, how = 'left',left_on = ['air_store_id', 'non_working_day'],right_on = ['air_store_id', 'non_working_day'])
    return data

"""###### *Latitude and Longitude*"""

# calculate the sum of latitude and longitude which is then normalized
# calculate the difference between max latitude( or longitude )
def longitude_latitude_calc(data):
        
    data['longitude_latitude_sum'] = data['latitude'] + data['longitude']
    data['longitude_latitude_sum'] = data['longitude_latitude_sum'].apply(lambda x: np.log1p(x)) 
    
    data['diff_max_latitude'] = data['latitude'].max() - data['latitude']
    data['diff_max_longitude'] = data['longitude'].max() - data['longitude']
    
    return data

"""###### *Date Information*"""

# convert air store id + visit date into two separate columns
# divide date into separate features: date, month, day of week, week of year
def get_id_and_visit_date_details(X):    
    
    X['id'] = X['air_store_id'] + str('_') + X['visit_date']
    
    X['visit_date'] = pd.to_datetime(X['visit_date'],format= '%Y-%m-%d')
    X['visit_date_month'] =X.visit_date.dt.month
    X['visit_date_dayofw'] =X.visit_date.dt.dayofweek
    X['visit_date_year'] =X.visit_date.dt.year
    X['visit_date_dayofm'] =X.visit_date.dt.day
    X['weekofyear'] =X.visit_date.dt.weekofyear
    X.loc[X.weekofyear==53,'weekofyear'] =0 
    return X

"""###### *Merging features created till now*"""

# Merge all the feature engineered data
def merge_fe_data(data):
    # add month of visit
    data['month'] = data['visit_date'].apply(lambda x: float(str(x)[5:7]))

    # add weekday and holiday flag
    data = data.join(date_info_mod.set_index('calendar_date'), on = 'visit_date')
    # add genre and area name)
    data = data.join(air_store_info.set_index('air_store_id'), on = 'air_store_id')
    # add quantity of same genre in area
    data = merge_genres_area(data);
     # add total quatity of restaurants in area
    data = merge_total_rt_in_area(data);
   
    # add reservation information
    data = pd.merge(data, reserve, how = 'left', left_on = ['air_store_id', 'visit_date'],right_on = ['air_store_id', 'visit_date'])
    
    # add visitors number mean, median, max for each restaurant based on non-working day
    data = merge_calc_wd_data(data, mean_of_visitors)
    data = merge_calc_wd_data(data, median_of_visitors)
    data = merge_calc_wd_data(data, max_visitors)

    # add latitude and longitude sum and difference
    data = longitude_latitude_calc(data)
   
    # change NaN to 0
    data = data.fillna(0) 
    return data

# combine train/test data with additional information
air_train = air_visit_data.copy()
X = merge_fe_data(air_train)
X = get_id_and_visit_date_details(X)
X.head()

# Merge the test data as well
air_test = sample_sub.copy()
air_test['air_store_id'] = air_test['id'].apply(lambda x: str(x)[:-11])
air_test['visit_date'] = air_test['id'].apply(lambda x: str(x)[-10:])
X_test = merge_fe_data(air_test)
X_test=get_id_and_visit_date_details(X_test)
X_test.head(5)

"""###### *Mean Median and Max Visitors based on different features*"""

# create copy of train and test set created till now
k = [i for i in X.columns if i in X_test.columns]
train = X.copy()
test = X_test.copy()

# normalize the columns that have large values
def normalize_data(data, col_name) :
  data[col_name] = data[col_name].apply(lambda x: np.log1p(x))
  return data

# Find the mean, median and max visitors of stores based on day of week, month
# CHANGES MADE - previously, we considered mean, median and max visitors according to month and day of week
# Now, we consider these values according to month and day of week for EACH store id, instead of simply calculating overall value.

k1 = train[['visitors','air_store_id','visit_date_dayofw']].groupby(['air_store_id','visit_date_dayofw']).agg('mean').reset_index()
k1.columns = ['air_store_id','visit_date_dayofw','store_dayofw_mean_visitors']

k2 = train[['visitors','air_store_id','visit_date_dayofw']].groupby(['air_store_id','visit_date_dayofw']).agg('median').reset_index()
k2.columns = ['air_store_id','visit_date_dayofw','store_dayofw_median_visitors']

k3 = train[['visitors','air_store_id','visit_date_dayofw']].groupby(['air_store_id','visit_date_dayofw']).agg('max').reset_index()
k3.columns = ['air_store_id','visit_date_dayofw','store_dayofw_max_visitors']

k7 = train[['visitors','air_store_id']].groupby('air_store_id').agg('mean').reset_index()
k7.columns = ['air_store_id','store_mean_visitors']

k8 = train[['visitors','air_store_id']].groupby('air_store_id').agg('median').reset_index()
k8.columns = ['air_store_id','store_median_visitors']

k9 = train[['visitors','air_store_id']].groupby('air_store_id').agg('max').reset_index()
k9.columns = ['air_store_id','store_max_visitors']

k13 = train[['visitors','air_store_id','visit_date_month']].groupby(['air_store_id','visit_date_month']).agg('mean').reset_index()
k13.columns = ['air_store_id','visit_date_month','store_month_mean_visitors']

k14 = train[['visitors','air_store_id','visit_date_month']].groupby(['air_store_id','visit_date_month']).agg('median').reset_index()
k14.columns = ['air_store_id','visit_date_month','store_month_median_visitors']

k15 = train[['visitors','air_store_id','visit_date_month']].groupby(['air_store_id','visit_date_month']).agg('max').reset_index()
k15.columns = ['air_store_id','visit_date_month','store_month_max_visitors']

# Merge all the newly created min max visitors with train and test set

train = X.copy()
test = X_test.copy()
train = train[k]
test = test[k]

train = train.merge(k1,on=['air_store_id','visit_date_dayofw'],how='left')
test = test.merge(k1,on= ['air_store_id','visit_date_dayofw'],how='left')

train = train.merge(k2,on=['air_store_id','visit_date_dayofw'],how='left')
test = test.merge(k2,on= ['air_store_id','visit_date_dayofw'],how='left')

train = train.merge(k3,on=['air_store_id','visit_date_dayofw'],how='left')
test = test.merge(k3,on= ['air_store_id','visit_date_dayofw'],how='left')


train = train.merge(k7,on=['air_store_id'],how='left')
test = test.merge(k7,on= ['air_store_id'],how='left')

train = train.merge(k8,on=['air_store_id'],how='left')
test = test.merge(k8,on= ['air_store_id'],how='left')

train = train.merge(k9,on=['air_store_id'],how='left')
test = test.merge(k9,on= ['air_store_id'],how='left')


train = train.merge(k13,on=['air_store_id','visit_date_month'],how='left')
test = test.merge(k13,on= ['air_store_id','visit_date_month'],how='left')

train = train.merge(k14,on=['air_store_id','visit_date_month'],how='left')
test = test.merge(k14,on= ['air_store_id','visit_date_month'],how='left')

train = train.merge(k15,on=['air_store_id','visit_date_month'],how='left')
test = test.merge(k15,on= ['air_store_id','visit_date_month'],how='left')

train.replace(np.nan,-1,inplace=True)
test.replace(np.nan,-1,inplace=True)

"""###### *Area and Genre - One hot encoding*"""

# split the area name and create new columns
# only first part of area will be used
def split_area(df):
  df[['area1','area2','area3','4','5']]=df.air_area_name.str.split(pat=' ',expand=True)
  df.drop(columns=['area3','4','5'],axis=1,inplace=True)
  return df

train=split_area(train)
test=split_area(test)

# one hot encoding of the area and genre columns for train set
dum=pd.get_dummies(train.area1)
dum2=pd.get_dummies(train.air_genre_name)
frames=[train,dum,dum2]
train=pd.concat(frames,axis=1)

# one hot encoding of the area and genre columns for train set
dum=pd.get_dummies(test.area1)
dum2=pd.get_dummies(test.air_genre_name)
frames=[test,dum,dum2]
test=pd.concat(frames,axis=1)

# drop columns not necessary for train set
k = ['air_store_id', u'visit_date', u'month', u'day_of_week', u'air_area_name', 'visit_date_year', u'latitude', u'longitude']
X = train.drop(k,axis=1)
X.head(5)

# drop unnecessary columns for test
X_test = test.drop(k,axis=1)
X_test.head(5)

# save newly created train and test set
X.to_csv('new_train.csv',index = False)
X_test.to_csv('new_test.csv',index = False)

"""## Training

##### *Final train test set creation*
"""

# Reading the train and  test data from new_train. and new_test.csv files respectively
ntrain = pd.read_csv(path + 'new_train.csv')
ntest = pd.read_csv(path +'new_test.csv')

# dropping id and visitors columns from test dataset and id column from train dataset
ntest_1=ntest.drop(['id','visitors'],axis=1)
ntrain_1=ntrain.drop(['id'],axis=1)

# selecting final features to train on
final_cols=['holiday_eve', 'non_working_day',
       'genre_in_area', 'total_rt_in_area', 'reserve_visitors',
       'reserve_-12_h', 'reserve_12_37_h', 'reserve_37_59_h',
       'reserve_59_85_h', 'reserve_85+_h', 'nwd_mean_visitors',
       'nwd_median_visitors', 'nwd_max_visitors', 'longitude_latitude_sum',
       'diff_max_latitude', 'diff_max_longitude', 'visit_date_month',
       'visit_date_dayofw', 'visit_date_dayofm', 'weekofyear',
       'store_dayofw_mean_visitors', 'store_dayofw_median_visitors',
       'store_dayofw_max_visitors',  'store_mean_visitors',
       'store_median_visitors', 'store_max_visitors', 'store_month_mean_visitors', 
       'store_month_median_visitors','store_month_max_visitors',
       'Fukuoka-ken', 'Hiroshima-ken', 'Hokkaidō', 'Hyōgo-ken', 'Miyagi-ken',
       'Niigata-ken', 'Shizuoka-ken', 'Tōkyō-to', 'Ōsaka-fu', 'Asian',
       'Bar/Cocktail', 'Cafe/Sweets', 'Creative cuisine', 'Dining bar',
       'International cuisine', 'Italian/French', 'Izakaya', 'Japanese food',
       'Karaoke/Party', 'Okonomiyaki/Monja/Teppanyaki', 'Other',
       'Western food', 'Yakiniku/Korean food']

y = ntrain_1['visitors']
x = ntrain_1[final_cols]
test_x = ntest_1[final_cols]
test_x.head()

"""##### *Linear Regression*"""

from sklearn import datasets, linear_model

# Create linear regression object
regr = linear_model.LinearRegression()

# Train the model using the training sets
# x = features used for training
# y = Target(ie- no of visitors)
regr.fit(x, y)

# Make predictions using the testing set
y_pred = regr.predict(test_x)

# create final_submissions dataframe out of y_pred array
final_submissions = ntest[['id','visitors']]
final_submissions['id'] = ntest[['id']]
final_submissions['visitors'] = pd.DataFrame({'visitors': y_pred})

final_submissions = final_submissions.reindex(['id','visitors'],axis=1)

#Generate a submission csv file out of the predictions obtained from linear regression
final_submissions.to_csv('LinearRegression.csv',index = False)

"""##### *Decision Tree Regressor - Hyper parameter Tuning with GridSearch and Cross Validation of 5*"""

# Hyper parameters for Decision Tree Regressor are max_depth and min_samples_split
# njobs = -1 which means that all threads are run parallelly

from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeRegressor

model = DecisionTreeRegressor()
gs = GridSearchCV(model,
                  param_grid = {'max_depth': range(1, 11),
                                'min_samples_split': range(10, 60, 10)},
                  cv=5,
                  n_jobs=-1,
                  scoring='neg_mean_squared_log_error')

#Fit the model
gs.fit(x,y)
print(gs.best_params_)
print(-gs.best_score_)

# Make predictions using the Decision Tree Regressor
y_pred_tree = gs.predict(test_x)

# create final_submissions dataframe out of y_pred_tree array
final_submissions_tree = ntest[['id','visitors']]
final_submissions_tree['id'] = ntest[['id']]
final_submissions_tree['visitors'] = pd.DataFrame({'visitors': y_pred_tree})
final_submissions_tree = final_submissions_tree.reindex(['id','visitors'],axis=1)
final_submissions_tree.head()

#Generate a submission csv file out of the predictions obtained from Decision Tree Regressor
final_submissions_tree.to_csv('DecisionTree.csv',index = False)

"""##### *K nearest neighbors*"""

model1 = KNeighborsRegressor(n_jobs=-1,n_neighbors=4)
print(model1)

#Fit Knn model
model1.fit(x, y)
preds1 = model1.predict(x)
print('RMSLE KNeighborsRegressor: ', mean_squared_log_error(y, preds1))
preds1 = model1.predict(test_x)

# create final_submissions dataframe out of preds1 array
final_submissions_knn_1 = ntest[['id','visitors']]
final_submissions_knn_1['id'] = ntest[['id']]
final_submissions_knn_1['visitors'] = pd.DataFrame({'visitors': preds1})
final_submissions_knn_1 = final_submissions_knn_1.reindex(['id','visitors'],axis=1)
final_submissions_knn_1.head()

final_submissions_knn_1.to_csv('Knn_k=4.csv',index = False)

# Knn with neighbors=3 
from sklearn.metrics import mean_squared_log_error
from sklearn.neighbors import KNeighborsRegressor

model2 = KNeighborsRegressor(n_jobs=-1,n_neighbors=3)
print(model2)

model2.fit(x, y)
preds2 = model2.predict(x)
print('RMSLE KNeighborsRegressor: ', mean_squared_log_error(y, preds2))
preds2 = model2.predict(test_x)

# create final_submissions dataframe out of preds2 array
final_submissions_knn = ntest[['id','visitors']]
final_submissions_knn['id'] = ntest[['id']]
final_submissions_knn['visitors'] = pd.DataFrame({'visitors': preds2})
final_submissions_knn = final_submissions_knn.reindex(['id','visitors'],axis=1)
final_submissions_knn.head()

#Generate a submission csv file out of the predictions obtained from Knn
final_submissions_knn.to_csv('Knn_k=3.csv',index = False)

"""##### *XgBoost Model using train,test and split method to split train dataset into Training and Validation Datasets*"""

# Splitting 
train_X, test_valid_x, train_y, test_valid_y = train_test_split(x, y, test_size = 0.3, random_state = 123) 
  
# Instantiation 
xgb_r = xg.XGBRegressor(objective ='reg:linear', n_estimators = 10, seed = 123) 
  
# Fitting the model 
xgb_r.fit(train_X, train_y) 
  
# Predict the model 
pred = xgb_r.predict(test_valid_x) 
  
# RMSE Computation 
rmse = np.sqrt(mean_squared_log_error(test_valid_y, pred)) 
print("RMSE : % f" %(rmse))

y_pred_xgboost = xgb_r.predict(test_x)

final_submissions_xgboost = ntest[['id','visitors']]
final_submissions_xgboost['id'] = ntest[['id']]
final_submissions_xgboost['visitors'] = pd.DataFrame({'visitors': y_pred_xgboost})
final_submissions_xgboost = final_submissions_xgboost.reindex(['id','visitors'],axis=1)
final_submissions_xgboost.head()

final_submissions_xgboost.to_csv('XGBoost.csv',index = False)

"""##### *Light Gradient Boosting Model*"""

import lightgbm
# Light GBM Train on whole data set -
parameters = {}

# Create the LightGBM data containers

train_data = lightgbm.Dataset(x, label=y)
model = lightgbm.train(parameters,train_data)

# Predict the model 
y_pred_lgbm = model.predict(test_x)

# Genrating Submission For Light GBM without train_test_split
final_submissions_lgbm_without = ntest[['id','visitors']]
final_submissions_lgbm_without['id'] = ntest[['id']]
final_submissions_lgbm_without['visitors'] = pd.DataFrame({'visitors': y_pred_lgbm})
final_submissions_lgbm_without = final_submissions_lgbm_without.reindex(['id','visitors'],axis=1)
final_submissions_lgbm_without.head()

final_submissions_lgbm_without.to_csv('LightGradientBoosting.csv',index = False)

"""##### *Random Forest Regressor*"""

rf = RandomForestRegressor(max_depth=8, n_estimators=500,random_state=42,verbose=False)
rf.fit(x,y)

# Predict the model 
y_pred_rf = rf.predict(test_x)

# Genrating Submission For Random Forest without train_test_split
final_submissions_rf = ntest[['id','visitors']]
final_submissions_rf['id'] = ntest[['id']]
final_submissions_rf['visitors'] = pd.DataFrame({'visitors': y_pred_rf})
final_submissions_rf = final_submissions_rf.reindex(['id','visitors'],axis=1)
final_submissions_rf.head()

final_submissions_rf.to_csv('RandomForest.csv',index = False)

